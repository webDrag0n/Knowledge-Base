## Q1
- The covariance matrix \( S \) is computed as:
  \[
  S = \frac{1}{n-1} X^T X
  \]
- Next,
  \[
  S \mathbf{u}_i = \lambda_i \mathbf{u}_i \quad \text{for} \quad i = 1, 2, \dots, k
  \]
  Where \( \mathbf{u}_i \) are the eigenvectors of \( S \), and \( \lambda_i \) are the corresponding eigenvalues.
- Kernel matrix \( K \) is computed as:
  \[
  K = \Phi(X) \Phi(X)^T
  \]
  Where \( \Phi(X) \) is the feature mapping (implicit in kernel methods).
- Given that \( K \) is a Gram matrix formed by \( \Phi(X) \), the kernel matrix eigenvectors \( \mathbf{v}_i \) can be related to the eigenvectors \( \mathbf{u}_i \) of the covariance matrix \( S \).
- Next,
  \[
  \mathbf{v}_i = X \mathbf{u}_i
  \]
  Where \( \mathbf{u}_i \) is the eigenvector of \( S \), and \( \mathbf{v}_i \) is the corresponding eigenvector of the kernel matrix \( K \).
- ## Q2
- The objective is to solve:
  \[
  \min_{U,V} \| X - UV^T \|_F^2
  \]
  where \( X \in \mathbb{R}^{d \times N} \), \( U \in \mathbb{R}^{d \times k} \), and \( V \in \mathbb{R}^{N \times k} \), with \( k \) being the number of principal components.
  
  \[
  \| X - UV^T \|_F^2 = \text{trace}\left( (X - UV^T)^T (X - UV^T) \right).
  \]
  
  \[
  \| X - UV^T \|_F^2 = \text{trace}(X^T X) - 2 \text{trace}(V^T X^T U) + \text{trace}(V^T U^T U V).
  \]
  Using the assumption \( U^T U = I \)
  \[
  \| X - UV^T \|_F^2 = \text{trace}(X^T X) - 2 \text{trace}(V^T X^T U) + \text{trace}(V^T V).
  \]
- By applying the SVD to \( X \), we have:
  \[
  X = U \Sigma V^T,
  \]
  Thus, \( X^T X = V \Sigma^2 V^T \) and \( XX^T = U \Sigma^2 U^T \).
- The matrices \( U \) and \( V \) that minimize the objective function correspond to the left and right singular vectors of \( X \), respectively. Since \( X \) is centered, the covariance matrix \( C = XX^T \) is symmetric, and its eigenvectors are the left singular vectors of \( X \).
  
  Thus, the columns of \( U \) are the eigenvectors of the covariance matrix \( C = XX^T \), and the associated eigenvalues are the corresponding singular values squared.
- ## Q3
- We compute \( q^T D q \):
  \[
  q^T D q = \sum_{i \in A} d_i q(i)^2 + \sum_{i \in B} d_i q(i)^2
  \]
  For \( i \in A \), \( q(i) = \sqrt{\frac{d_B}{d_A d}} \), so:
  \[
  q(i)^2 = \frac{d_B}{d_A d}
  \]
  For \( i \in B \), \( q(i) = -\sqrt{\frac{d_A}{d_B d}} \), so:
  \[
  q(i)^2 = \frac{d_A}{d_B d}
  \]
  Thus:
  \[
  q^T D q = \sum_{i \in A} d_i \frac{d_B}{d_A d} + \sum_{i \in B} d_i \frac{d_A}{d_B d}
  \]
  Since \( d_A = \sum_{i \in A} d_i \) and \( d_B = \sum_{i \in B} d_i \), this simplifies to:
  \[
  q^T D q = \frac{d_B}{d_A d} d_A + \frac{d_A}{d_B d} d_B = \frac{d_B}{d} + \frac{d_A}{d} = 1
  \]
- Next,
  \[
  q^T D e = \sum_{i} d_i q(i)
  \]
  For \( i \in A \), \( q(i) = \sqrt{\frac{d_B}{d_A d}} \), and for \( i \in B \), \( q(i) = -\sqrt{\frac{d_A}{d_B d}} \). Thus:
  \[
  q^T D e = \sqrt{\frac{d_B}{d_A d}} \sum_{i \in A} d_i - \sqrt{\frac{d_A}{d_B d}} \sum_{i \in B} d_i
  \]
  Using \( d_A = \sum_{i \in A} d_i \) and \( d_B = \sum_{i \in B} d_i \), we get:
  \[
  q^T D e = \sqrt{\frac{d_B}{d_A d}} d_A - \sqrt{\frac{d_A}{d_B d}} d_B = \sqrt{\frac{d_B}{d_A d}} d_A - \sqrt{\frac{d_A}{d_B d}} d_B = 0
  \]
  Thus, \( q^T D e = 0 \).
- Next, we compute the normalized cut objective \( J_{Ncut}(q) \) using \( q \):
  \[
  J_{Ncut}(q) = \frac{s(A,B)}{d_A} + \frac{s(A,B)}{d_B}
  \]
  We can express \( s(A,B) \) as:
  \[
  s(A,B) = \sum_{i \in A} \sum_{j \in B} w_{ij}
  \]
  Thus:
  \[
  J_{Ncut}(q) = \frac{\sum_{i \in A} \sum_{j \in B} w_{ij}}{d_A} + \frac{\sum_{i \in A} \sum_{j \in B} w_{ij}}{d_B}
  \]
  By the definition of \( q(i) \), we can rewrite the above in matrix form:
  \[
  J_{Ncut}(q) = q^T (D - W) q
  \]
  This confirms the result \( J_{Ncut}(q) = q^T (D - W) q \).
- The optimization problem becomes:
  \[
  \min q^T (D - W) q + \lambda (q^T D q - 1)
  \]
  Since \( q^T D q = 1 \), the Lagrange multiplier term becomes zero, and the problem reduces to:
  \[
  \min q^T (D - W) q
  \]
  This is equivalent to solving the generalized eigenvalue problem:
  \[
  (D - W) q = \lambda D q
  \]
- ## Q4
- compute \( q^T D q \):
  \[
  q^T D q = \sum_{i \in A} d_i q(i)^2 + \sum_{i \in B} d_i q(i)^2
  \]
  For \( i \in A \), \( q(i) = \sqrt{\frac{d_B}{d_A d}} \), so:
  \[
  q(i)^2 = \frac{d_B}{d_A d}
  \]
  For \( i \in B \), \( q(i) = -\sqrt{\frac{d_A}{d_B d}} \), so:
  \[
  q(i)^2 = \frac{d_A}{d_B d}
  \]
  Thus:
  \[
  q^T D q = \sum_{i \in A} d_i \frac{d_B}{d_A d} + \sum_{i \in B} d_i \frac{d_A}{d_B d}
  \]
  Since \( d_A = \sum_{i \in A} d_i \) and \( d_B = \sum_{i \in B} d_i \), this simplifies to:
  \[
  q^T D q = \frac{d_B}{d_A d} d_A + \frac{d_A}{d_B d} d_B = \frac{d_B}{d} + \frac{d_A}{d} = 1
  \]
- Next, compute \( q^T D e \):
  \[
  q^T D e = \sum_{i} d_i q(i)
  \]
  For \( i \in A \), \( q(i) = \sqrt{\frac{d_B}{d_A d}} \), and for \( i \in B \), \( q(i) = -\sqrt{\frac{d_A}{d_B d}} \). Thus:
  \[
  q^T D e = \sqrt{\frac{d_B}{d_A d}} \sum_{i \in A} d_i - \sqrt{\frac{d_A}{d_B d}} \sum_{i \in B} d_i
  \]
  Using \( d_A = \sum_{i \in A} d_i \) and \( d_B = \sum_{i \in B} d_i \), we get:
  \[
  q^T D e = \sqrt{\frac{d_B}{d_A d}} d_A - \sqrt{\frac{d_A}{d_B d}} d_B = \sqrt{\frac{d_B}{d_A d}} d_A - \sqrt{\frac{d_A}{d_B d}} d_B = 0
  \]
  Thus, \( q^T D e = 0 \).
  
  \[
  J_{Ncut}(q) = \frac{s(A,B)}{d_A} + \frac{s(A,B)}{d_B}
  \]
  We can express \( s(A,B) \) as:
  \[
  s(A,B) = \sum_{i \in A} \sum_{j \in B} w_{ij}
  \]
  Thus:
  \[
  J_{Ncut}(q) = \frac{\sum_{i \in A} \sum_{j \in B} w_{ij}}{d_A} + \frac{\sum_{i \in A} \sum_{j \in B} w_{ij}}{d_B}
  \]
  By the definition of \( q(i) \), we can rewrite the above in matrix form:
  \[
  J_{Ncut}(q) = q^T (D - W) q
  \]
  This confirms the result \( J_{Ncut}(q) = q^T (D - W) q \).
- The optimization problem becomes:
  \[
  \min q^T (D - W) q + \lambda (q^T D q - 1)
  \]
  Since \( q^T D q = 1 \), the Lagrange multiplier term becomes zero, and the problem reduces to:
  \[
  \min q^T (D - W) q
  \]
  This is equivalent to solving the generalized eigenvalue problem:
  \[
  (D - W) q = \lambda D q
  \]
-