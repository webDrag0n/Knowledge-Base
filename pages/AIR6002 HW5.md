## Q1
- ### (i) **Details of the Two Steps in K-means Clustering**
  1. **Cluster Assignment Step**:
	- Mathematically written as:
	  \[
	  \text{Assign point } x_i \text{ to cluster } C_k \text{ if } \| x_i - \mu_k \|^2 \text{ is minimized}
	  \]
	  where \( \mu_k \) is the centroid of cluster \( k \).
- 2. **Centroid Update Step**:
	- After all points are assigned to clusters, the centroids are updated.
	- The centroid of each cluster is updated as the mean of all points assigned to that cluster.
	- If a point \( x_i \) is assigned to cluster \( C_k \), the new centroid \( \mu_k^{new} \) is computed as:
	  \[
	  \mu_k^{new} = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
	  \]
	  where \( |C_k| \) is the number of points in cluster \( C_k \).
	  ---
- ### (ii) **Computational Complexity of Each Step**
  
  1. **Step 1: Cluster Assignment**:
	- The distance calculation between a point \( x_i \) (with \( d \) features) and a centroid \( \mu_k \) is \( O(d) \), where \( d \) is the number of attributes.
	- There are \( K \) centroids to compare, so the complexity for one data point is \( O(K \times d) \).
	- For \( n \) data points, the total complexity of the cluster assignment step is:
	  \[
	  O(n \times K \times d)
	  \]
- 2. **Step 2: Centroid Update**:
	- For each cluster \( k \), the centroid is the average of the data points in that cluster. Summing the points in each cluster requires \( O(d) \) operations for each cluster.
	- The total complexity of updating all \( K \) centroids is:
	  \[
	  O(n \times d)
	  \]
	  ---
- ### (iii) **Proving the Overall Complexity**
  
  1. **Cluster Assignment Step Complexity**:
	- The time complexity for the cluster assignment step is \( O(n \times K \times d) \).
	- This is because for each of the \( n \) data points, you compute the distance to each of the \( K \) centroids, and each distance computation takes \( O(d) \) time (since the points are \( d \)-dimensional).
- 2. **Centroid Update Step Complexity**:
	- The time complexity for the centroid update step is \( O(n \times d) \).
	- This is because we need to process all \( n \) points to update the centroids (one per cluster).
	  Thus, for **one iteration**, the total time complexity is:
	  \[
	  O(n \times K \times d) + O(n \times d) = O(n \times K \times d)
	  \]
	  because \( O(n \times K \times d) \) dominates \( O(n \times d) \).
- 3. **Total Complexity for \( I \) Iterations**:
	- If the algorithm runs for \( I \) iterations, the total complexity will be:
	  \[
	  O(I \times n \times K \times d)
	  \]
	  where \( I \) is the number of iterations, \( n \) is the number of data points, \( K \) is the number of clusters, and \( d \) is the number of attributes.
	  Thus, the total time complexity of the K-means algorithm is:
	  \[
	  O(I \times n \times K \times d)
	  \]
	  This matches the complexity given in the problem.
	  ---
- ## Q2
- ### Difference Between Batch and Online K-means Algorithms
  
  In K-means clustering, there are two primary approaches to updating the centroids: the **Batch algorithm** and the **Online algorithm**. Both aim to assign data points to clusters and update the centroids iteratively, but they differ in how the centroids are updated during the clustering process.
- #### 1. **Batch Algorithm (Standard K-means)**
	- In the **batch algorithm**, the entire dataset is processed in a two-step process for each iteration:
	- 1. **Cluster Assignment**:
		- In this step, each data point is assigned to the cluster whose centroid is closest. The assignment is based on the Euclidean distance (or another distance metric) between each data point \( x_i \) and the centroids \( \mu_k \) of the \( K \) clusters.
		- Mathematically, for each data point \( x_i \), you assign it to cluster \( k_i^t \), where:
		  \[
		  k_i^t = \arg\min_{k} \| x_i - \mu_k^t \|^2
		  \]
		  Here, \( \mu_k^t \) is the centroid of the \( k \)-th cluster at iteration \( t \).
	- 2. **Centroid Update**:
		- After all points have been assigned to clusters, the centroids are updated. The new centroid for each cluster is the mean of all the points assigned to that cluster:
		  \[
		  \mu_k^{t+1} = \frac{1}{|S_k^t|} \sum_{x_i \in S_k^t} x_i
		  \]
		  where \( S_k^t \) is the set of points assigned to cluster \( k \) at iteration \( t \), and \( |S_k^t| \) is the number of points in cluster \( k \).
		- This process continues iteratively, with the cluster assignments being recalculated after each centroid update.
- #### 2. **Online Algorithm (Stochastic K-means)**
	- In contrast, the **online algorithm** updates the centroids after each data point is assigned to a cluster. The key steps in the online algorithm are as follows:
	- 1. **Cluster Assignment and Centroid Update**:
		- For each data point \( x_i \), the algorithm assigns the point to the nearest cluster (same as in the batch algorithm), but then immediately updates the centroids of both the **new cluster** and the **previous cluster** (if the pointâ€™s assignment has changed).
		- Suppose at time \( t \), point \( x_i \) is assigned to cluster \( k_i^t \). If this assignment differs from the previous cluster \( k_i^{t-1} \), both the centroid of the current cluster \( k_i^t \) and the previous cluster \( k_i^{t-1} \) are updated. The update rule for the centroids can be written as:
		  \[
		  \mu_k^{t+1} = \mu_k^t + \alpha \left( x_i - \mu_k^t \right)
		  \]
		  where \( \alpha \) is the learning rate (a small positive constant).
	- 2. **Order Dependence**:
		- Unlike the batch algorithm, where the assignments and updates are decoupled (all points are assigned first, then all centroids are updated), the online algorithm updates centroids immediately as each data point is processed.
		- The order in which data points are presented affects the final clustering result, as different orders of processing may lead to different centroid updates.
- ### Properties of Batch vs Online K-means Algorithm
  
  Based on the differences between these two approaches, we can explain the four properties mentioned in the problem statement:
- #### 1. **More Expensive (Online Algorithm)**
  The **online algorithm** is generally more computationally expensive than the batch algorithm. This is because, in each iteration, every data point is processed individually, and the centroids of both the current and previous clusters must be updated after each assignment. As a result, centroid updates occur more frequently, and this leads to a higher computational cost per iteration, especially for large datasets. In contrast, the batch algorithm only updates the centroids once all points are assigned to their respective clusters, reducing the frequency of updates and computational expense.
- #### 2. **Introduces Order Dependency (Online Algorithm)**
  The **online algorithm** is order-dependent, meaning that the sequence in which data points are processed can influence the final clustering result. Since centroids are updated after each assignment, the path taken through the data points matters. For example, if the first few points that are processed happen to belong to different clusters than their final assignments, the centroids will evolve differently compared to when the points are processed in a different order. On the other hand, the **batch algorithm** does not suffer from order dependency because all points are processed in parallel before any centroids are updated, meaning the final result is independent of the order in which the data points are considered.
- #### 3. **Never Gets an Empty Cluster (Online Algorithm)**
  In the **online algorithm**, it is unlikely for any cluster to become empty. Since centroids are updated after every data point is assigned, the algorithm ensures that the centroids remain "alive" and continuously evolve based on the incoming points. Even if a centroid temporarily has no points assigned to it, once new data points are processed, they may eventually be assigned to that cluster, preventing it from becoming empty. 
  
  In contrast, in the **batch algorithm**, it is possible for a cluster to remain empty if, during the assignment step, no data points are closest to the centroid of that cluster. This situation can occur, especially in cases where the centroids are initialized poorly or if the number of clusters is too large for the given data.
- #### 4. **Generally Improves Clustering Results (Online Algorithm)**
  The **online algorithm** can generally improve the clustering results compared to the batch algorithm. Since centroids are updated more frequently, they can more quickly adapt to changes in the dataset. This means that the algorithm is more responsive to local patterns and outliers in the data, and the centroids can adjust incrementally as more points are processed. This can lead to a better overall clustering, especially when dealing with non-stationary data or streaming data.
  
  On the other hand, the **batch algorithm** may take longer to converge to an optimal solution, as it only updates centroids after all points are assigned in each iteration. This slower adjustment process can sometimes result in suboptimal clustering, especially if the data is complex or contains significant noise.
  ---
- ## Q3
- ### A. Proof of the Theorem
	- The **hard EM** algorithm maximizes the **log-likelihood** of the mixture model, which for a Gaussian mixture model (GMM) is:
	  
	  \[
	  \log P(X \mid \theta) = \sum_{i=1}^{N} \log \sum_{k=1}^{K} \pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)
	  \]
	  
	  \[
	  \gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}
	  \]
	  The **M-step** involves updating the parameters by maximizing the expected log-likelihood with respect to the responsibilities \( \gamma_{ik} \).
	- Given the assumption that the covariance matrices are spherical, we have:
	  
	  \[
	  \Sigma_k = I \quad \forall k
	  \]
	  
	  This assumption simplifies the Gaussian distribution to a diagonal matrix with unit variance in all directions. The multivariate Gaussian probability density function becomes:
	  
	  \[
	  \mathcal{N}(x_i \mid \mu_k, I) = \frac{1}{(2\pi)^{d/2}} \exp \left( -\frac{1}{2} \| x_i - \mu_k \|^2 \right)
	  \]
	  
	  Thus, the log-likelihood simplifies to:
	  
	  \[
	  \log P(X \mid \theta) = -\frac{1}{2} \sum_{i=1}^{N} \sum_{k=1}^{K} \gamma_{ik} \| x_i - \mu_k \|^2
	  \]
	-
	- If the pooled covariance assumption is applied, each cluster shares the same covariance, which is the identity matrix. This further simplifies the likelihood since there is no need to compute separate covariance matrices for each cluster, making the Gaussian distribution for each cluster identical up to a shift by the mean \( \mu_k \).
	- The M-step in the **hard EM** algorithm now updates the parameters \( \mu_k \) and \( \pi_k \) by maximizing the expected log-likelihood. Since the covariance is identity, the responsibility function \( \gamma_{ik} \) now only depends on the Euclidean distance between \( x_i \) and \( \mu_k \).
	  
	  Maximizing the likelihood over the means \( \mu_k \) corresponds to assigning each data point \( x_i \) to the cluster whose mean \( \mu_k \) is closest (in terms of Euclidean distance), which is the same step as in **k-means**. In k-means, the objective is to minimize the sum of squared distances between data points and their assigned cluster centroids.
	  
	  Thus, under the spherical covariance and pooled covariance assumptions, **hard EM** becomes identical to the **k-means** algorithm in terms of the assignment of points to clusters and the update of cluster centroids.
- ### B. Do we need to compute the covariance matrices? Why?
	- No, in this case, we do **not** need to compute the covariance matrices
	- The spherical covariance assumption implies that all clusters share the same covariance matrix, which is the identity matrix \( I \). This means that the covariance structure does not vary across clusters and is fixed.
	- The pooled covariance assumption, combined with the spherical assumption, essentially renders the covariance matrix irrelevant in terms of cluster-specific updates. The distance between points and cluster centroids is all that matters for the cluster assignments.
	  
	  Therefore, since the covariance matrix is assumed to be the identity matrix and does not change during the algorithm, it does not need to be computed or updated in each iteration. This simplifies the computations considerably, and the algorithm reduces to a simple k-means procedure.
- ---
- ## Q4